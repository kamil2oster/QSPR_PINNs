{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3941b1",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482b404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:03:01.016671Z",
     "start_time": "2024-06-12T09:02:59.487166Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccc877",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Firstly, the data prepared in Notebook 1 are loaded (data combined.csv), including dropping missing values (just in case any creeped in!).\n",
    "\n",
    "Next, we specify the inputs and outputs. Note that to_drop contains any columns that are not to be used in inputs. Inputs will only contain molecular descriptors, M, T and p.\n",
    "Outputs will be density values.\n",
    "\n",
    "Output is divided by 1000 to convert the unit from kg/m3 to g/cm3 (this will be important in PINN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294195f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:03:01.599726Z",
     "start_time": "2024-06-12T09:03:01.018134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/data combined.csv', index_col = 0).dropna(axis = 0)\n",
    "print('Loaded dataset:')\n",
    "display(df)\n",
    "\n",
    "extra_tags = ['M g/mol', 'T / K', 'p / MPa']\n",
    "to_drop = ['Dataset ID', 'IL ID', 'Cation', 'Anion', 'Cationic family', 'Anionic family',\n",
    "           'Excluded IL', 'Accepted dataset', 'T / K', 'p / MPa', 'ρ / kg/m3', 'SWMLR (v0) + FFANN (f)',\n",
    "           'SWMLR (v0) + FFANN (f).1', 'FFANN (v0) + FFANN (f)', 'FFANN (v0) + FFANN (f).1',\n",
    "           'LSSVM (v0) + FFANN (f)', 'LSSVM (v0) + FFANN (f).1', 'M g/mol']\n",
    "molecular_descriptors = df.drop(to_drop, axis = 1).columns\n",
    "inputs = np.hstack((molecular_descriptors, \n",
    "                    extra_tags))\n",
    "\n",
    "outputs = ['ρ / kg/m3']\n",
    "\n",
    "print(f'\\n\\nInputs {len(inputs)}:\\n{inputs}')\n",
    "print(f'\\n\\nOutputs {len(outputs)}:\\n{outputs}')\n",
    "\n",
    "X = df[inputs]\n",
    "y = df[outputs]/1000\n",
    "\n",
    "print('\\n\\nInput data:')\n",
    "display(X)\n",
    "\n",
    "print('\\n\\nOutput data:')\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2a6b8",
   "metadata": {},
   "source": [
    "# Scale and Split Data\n",
    "\n",
    "The code defines two functions, scale and descale, for normalising and denormalising data. The scale function adjusts the input data by subtracting a shift value and dividing by a factor for each column, effectively normalising the data. The descale function reverses this process, restoring the original data values by multiplying by the factor and adding the shift.\n",
    "\n",
    "In the preparation section, a dictionary named scaler is created to store the shift and factor values for both input features (X) and output values (y). These values are used to scale the inputs and outputs into a range defined by scaler_range.\n",
    "\n",
    "The scaled data is then stored in the dataset dictionary, and any columns with all NaN values are dropped. The dataset is split into training and testing sets using a 15% test size, with shapes of the resulting sets printed to confirm the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c1ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:03:01.816255Z",
     "start_time": "2024-06-12T09:03:01.601513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale(data_in, shift, factor):\n",
    "    \"\"\"\n",
    "    Scale the input data by shifting and dividing by a factor.\n",
    "    \n",
    "    Parameters:\n",
    "    data_in (array-like or DataFrame): The input data to be scaled.\n",
    "    shift (Series or DataFrame): The shift values for each column in the data.\n",
    "    factor (Series or DataFrame): The factor values for each column in the data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The scaled data.\n",
    "    \"\"\"\n",
    "    # Convert the input data to a DataFrame\n",
    "    data_in = pd.DataFrame(data_in)\n",
    "    \n",
    "    # Scale the data by subtracting the shift and dividing by the factor for each column\n",
    "    return (data_in - shift[data_in.columns]) / factor[data_in.columns]\n",
    "\n",
    "def descale(data_in, shift, factor):\n",
    "    \"\"\"\n",
    "    Descale the input data by multiplying by a factor and adding a shift.\n",
    "    \n",
    "    Parameters:\n",
    "    data_in (array-like or DataFrame): The input data to be descaled.\n",
    "    shift (Series or DataFrame): The shift values for each column in the data.\n",
    "    factor (Series or DataFrame): The factor values for each column in the data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The descaled data.\n",
    "    \"\"\"\n",
    "    # Convert the input data to a DataFrame\n",
    "    data_in = pd.DataFrame(data_in)\n",
    "    \n",
    "    # Descale the data by multiplying by the factor and adding the shift for each column\n",
    "    return data_in * factor[data_in.columns] + shift[data_in.columns]\n",
    "\n",
    "# Prepare scaler\n",
    "scaler = {}\n",
    "scaler_range = (0, 1)\n",
    "scaler['X_shift'] = X.min(axis=0)\n",
    "scaler['X_factor'] = X.max(axis=0) - X.min(axis=0)\n",
    "scaler['y_shift'] = y.min(axis=0)\n",
    "scaler['y_factor'] = y.max(axis=0) - y.min(axis=0)\n",
    "\n",
    "# Scale inputs X and output y\n",
    "dataset = {}\n",
    "dataset['X_scaled'] = scale(X, scaler['X_shift'], scaler['X_factor']).dropna(axis = 1, how = 'all')\n",
    "dataset['Y_scaled'] = scale(y, scaler['y_shift'], scaler['y_factor'])\n",
    "print('Scaled input data size:', dataset['X_scaled'].shape)\n",
    "print('Scaled outputs data size:', dataset['Y_scaled'].shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "dataset['X_train_scaled'], dataset['X_test_scaled'], dataset['Y_train_scaled'], dataset['Y_test_scaled'] = train_test_split(dataset['X_scaled'], dataset['Y_scaled'], test_size=0.15, random_state = 23)\n",
    "print('Training size:', dataset['X_train_scaled'].shape, dataset['Y_train_scaled'].shape)\n",
    "print('Testing size:', dataset['X_test_scaled'].shape, dataset['Y_test_scaled'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb750e",
   "metadata": {},
   "source": [
    "# Define Model and Training Functions\n",
    "\n",
    "This code defines a physics-informed neural network (PINN) model using PyTorch and includes functions for training, evaluating, and tracking the progress of the model.\n",
    "\n",
    "The PINNModel_torch class defines the architecture of the PINN model, which includes two hidden layers with 100 neurons each and multiple output layers for predicting density (predicted_density), base density (rho_0_pred), compressibility factor (k_pred), and specific volume (v_pred). The forward pass method applies ReLU activation functions to the outputs of the hidden layers and computes the final predicted values.\n",
    "\n",
    "The FTEoS function calculates the density using the Fluid Thermodynamic Equation of State (FTEoS) based on input parameters such as molecular weight (M), temperature (T), pressure (p), base density (rho_0), compressibility factor (k), and specific volume (v). The calculated density is scaled back to match the original data scale.\n",
    "\n",
    "The loss_fn function calculates the combined loss for the PINN model by computing the Mean Squared Error (MSE) between the model predictions and true values for both the neural network predictions and the FTEoS predictions. This combined loss function helps in guiding the training of the model.\n",
    "\n",
    "The closure function performs a single optimization step, including zeroing the gradients, computing the loss, performing backpropagation, and updating the model parameters.\n",
    "\n",
    "The get_metrics function calculates various performance metrics for the model, such as training and testing loss, Mean Squared Error (MSE), and Mean Absolute Error (MAE), and returns these metrics as a list.\n",
    "\n",
    "The print_progress function prints the progress of the training process, including the current epoch's training and testing loss, MSE, and MAE. It highlights epochs where a new best test loss is achieved and includes an asterisk for such epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4afbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:03:01.826788Z",
     "start_time": "2024-06-12T09:03:01.817561Z"
    }
   },
   "outputs": [],
   "source": [
    "class PINNModel_torch(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple physics-informed neural network (PINN) model using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dim (int): The number of input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(PINNModel_torch, self).__init__()\n",
    "        # Define the first fully connected layer with 100 neurons\n",
    "        self.dense1 = nn.Linear(input_dim, 100)\n",
    "        # Define the second fully connected layer with 100 neurons\n",
    "        self.dense2 = nn.Linear(100, 100)\n",
    "        # Define the output layers for different predicted values\n",
    "        self.dense_density = nn.Linear(100, 1)  # Predicted density\n",
    "        self.dense_rho0 = nn.Linear(100, 1)     # Predicted rho_0\n",
    "        self.dense_k = nn.Linear(100, 1)        # Predicted k\n",
    "        self.dense_v = nn.Linear(100, 1)        # Predicted v\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        Tuple[Tensor, Tensor, Tensor, Tensor]: Output tensors for predicted values.\n",
    "        \"\"\"\n",
    "        # Apply ReLU activation function to the output of the first layer\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        # Apply ReLU activation function to the output of the second layer\n",
    "        x = torch.relu(self.dense2(x))\n",
    "        # Generate the final outputs\n",
    "        predicted_density = self.dense_density(x)\n",
    "        rho_0_pred = self.dense_rho0(x)\n",
    "        k_pred = self.dense_k(x)\n",
    "        v_pred = self.dense_v(x)\n",
    "        return predicted_density, rho_0_pred, k_pred, v_pred\n",
    "\n",
    "def FTEoS(M, T, p, rho_0, k, v, p0=0.1, R=8.314):\n",
    "    \"\"\"\n",
    "    Calculate the density using the FTEoS (Fluid Thermodynamic Equation of State).\n",
    "    \n",
    "    Parameters:\n",
    "    M (Tensor): Molecular weight.\n",
    "    T (Tensor): Temperature.\n",
    "    p (Tensor): Pressure.\n",
    "    rho_0 (Tensor): Base density.\n",
    "    k (Tensor): Compressibility factor.\n",
    "    v (Tensor): Specific volume.\n",
    "    p0 (float): Reference pressure (default is 0.1).\n",
    "    R (float): Gas constant (default is 8.314).\n",
    "    \n",
    "    Returns:\n",
    "    Tensor: Scaled calculated density.\n",
    "    \"\"\"\n",
    "    # Flatten tensors\n",
    "    rho_0 = torch.flatten(rho_0)\n",
    "    k = torch.flatten(k)\n",
    "    v = torch.flatten(v)\n",
    "    # Convert scaled values to original values using factors and shifts\n",
    "    M = M * scaler['X_factor']['M g/mol'] + scaler['X_shift']['M g/mol']\n",
    "    T = T * scaler['X_factor']['T / K'] + scaler['X_shift']['T / K']\n",
    "    p = p * scaler['X_factor']['p / MPa'] + scaler['X_shift']['p / MPa']\n",
    "    # Calculate the density\n",
    "    density_calc = rho_0 + (1/k) * torch.log10(((M*k)/(v*R*T))*(p-p0)+1)\n",
    "    # Scale the calculated density\n",
    "    density_calc_scaled = (density_calc - scaler['y_shift']['ρ / kg/m3'])/scaler['y_factor']['ρ / kg/m3'] \n",
    "    return density_calc_scaled\n",
    "\n",
    "def loss_fn(model, X, y, M, T, p):\n",
    "    \"\"\"\n",
    "    Calculate the combined loss for the PINN model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (nn.Module): The PINN model.\n",
    "    X (Tensor): Input features.\n",
    "    y (Tensor): True density values.\n",
    "    M (Tensor): Molecular weight.\n",
    "    T (Tensor): Temperature.\n",
    "    p (Tensor): Pressure.\n",
    "    \n",
    "    Returns:\n",
    "    Tensor: Combined loss value.\n",
    "    \"\"\"\n",
    "    # Get model predictions\n",
    "    y_pred, rho_0_pred, k_pred, v_pred = model(X)\n",
    "    # Calculate MLP loss\n",
    "    mse_MLP = nn.functional.mse_loss(y_pred, y)\n",
    "    # Calculate FTEoS predictions and PINN loss\n",
    "    y_pred_EOS = FTEoS(M, T, p, rho_0_pred, k_pred, v_pred)\n",
    "    mse_PINN = nn.functional.mse_loss(y_pred_EOS, y)\n",
    "    # Return combined loss\n",
    "    return 0.5 * mse_MLP + 0.5 * mse_PINN\n",
    "\n",
    "def closure():\n",
    "    \"\"\"\n",
    "    Perform a single optimization step for the model.\n",
    "    \n",
    "    Returns:\n",
    "    Tensor: The loss value.\n",
    "    \"\"\"\n",
    "    # Zero the gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(model, x_batch, y_batch, M_batch, T_batch, p_batch)\n",
    "    loss = loss.mean()\n",
    "    # Backward pass: compute gradient of the loss with respect to parameters\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def get_metrics(model, X_train, X_test, y_train, y_test, loss_fn):\n",
    "    \"\"\"\n",
    "    Calculate and return various performance metrics for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (nn.Module): The trained model.\n",
    "    X_train (Tensor): Training input data.\n",
    "    X_test (Tensor): Testing input data.\n",
    "    y_train (Tensor): Training target data.\n",
    "    y_test (Tensor): Testing target data.\n",
    "    loss_fn (function): Loss function.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of metrics [train loss, test loss, train MSE, test MSE, train MAE, test MAE].\n",
    "    \"\"\"\n",
    "    # Use MSE loss function without reduction\n",
    "    mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "    # Predict on the training data\n",
    "    y_pred_train = model(X_train)[0]\n",
    "    # Compute the training loss\n",
    "    loss_train = mse_loss(y_pred_train, y_train)\n",
    "    # Predict on the testing data\n",
    "    y_pred_test = model(X_test)[0]\n",
    "    # Compute the testing loss\n",
    "    loss_test = mse_loss(y_pred_test, y_test)\n",
    "    # Calculate Mean Squared Error (MSE) for training data\n",
    "    MSE = torch.mean((y_pred_train - y_train) ** 2)\n",
    "    # Calculate Mean Absolute Error (MAE) for training data\n",
    "    MAE = torch.mean(torch.abs(y_pred_train - y_train))\n",
    "    # Calculate Mean Squared Error (MSE) for testing data\n",
    "    MSE_test = torch.mean((y_pred_test - y_test) ** 2)\n",
    "    # Calculate Mean Absolute Error (MAE) for testing data\n",
    "    MAE_test = torch.mean(torch.abs(y_pred_test - y_test))\n",
    "    # Return the computed metrics as a list\n",
    "    return [loss_train.mean().item(), \n",
    "            loss_test.mean().item(), \n",
    "            MSE.item(), \n",
    "            MSE_test.item(),\n",
    "            MAE.item(),\n",
    "            MAE_test.item()]\n",
    "\n",
    "def print_progress(epoch, loss_info, best_loss):\n",
    "    \"\"\"\n",
    "    Print the progress of training, including various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    epoch (int): The current epoch number.\n",
    "    loss_info (DataFrame): DataFrame containing the loss information.\n",
    "    best_loss (float): The best loss achieved so far.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the current epoch's test loss is better than the best loss\n",
    "    if loss_info.loc[epoch, 'loss test'] < best_loss:\n",
    "        # Print progress with an asterisk indicating a new best test loss\n",
    "        print('{}   {:.3e}/{:.3e}   {:.3e}/{:.3e}   {:.3e}/{:.3e} *'.format(epoch,\n",
    "            loss_info.loc[epoch, 'loss train'],\n",
    "            loss_info.loc[epoch, 'loss test'],\n",
    "            loss_info.loc[epoch, 'MSE train'], \n",
    "            loss_info.loc[epoch, 'MSE test'],\n",
    "            loss_info.loc[epoch, 'MAE train'], \n",
    "            loss_info.loc[epoch, 'MAE test']))\n",
    "    else:\n",
    "        # Print progress without an asterisk\n",
    "        print('{}   {:.3e}/{:.3e}   {:.3e}/{:.3e}   {:.3e}/{:.3e}    {}'.format(epoch,\n",
    "            loss_info.loc[epoch, 'loss train'],\n",
    "            loss_info.loc[epoch, 'loss test'],\n",
    "            loss_info.loc[epoch, 'MSE train'], \n",
    "            loss_info.loc[epoch, 'MSE test'],\n",
    "            loss_info.loc[epoch, 'MAE train'], \n",
    "            loss_info.loc[epoch, 'MAE test'],\n",
    "            epoch - last_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab013b9c",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "This code details the training and evaluation process for a Physics-Informed Neural Network (PINN) model using PyTorch.\n",
    "\n",
    "First, the code sets the computation device to GPU ('cuda') if available, for efficient training. The PINNModel_torch is initialized with the input feature dimension and moved to the GPU. The Adam optimizer is set up with a specified learning rate to update the model parameters during training.\n",
    "\n",
    "Training and testing data are converted into PyTorch tensors and moved to the GPU. A DataFrame is initialized to store various loss metrics, including training and testing loss, Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n",
    "\n",
    "Training parameters such as the number of epochs, early stopping patience, last improved epoch, batch size, and best loss are defined. The training loop begins by shuffling the training data for each epoch to ensure robust training. Mini-batch training is performed by dividing the data into batches and performing optimization steps on each batch. During each optimization step, the model parameters are updated based on the loss calculated using the closure function.\n",
    "\n",
    "After each epoch, performance metrics are calculated and stored. The training progress is printed, and the model is saved if the current test loss is the best seen so far. Early stopping is implemented to terminate training if no improvement in test loss is observed for a specified number of epochs (patience), preventing overfitting and saving computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77f981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:41:15.061640Z",
     "start_time": "2024-06-12T09:41:13.671605Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = 'cuda'\n",
    "\n",
    "# Initialize the PINN model and move it to the specified device\n",
    "#model = PINNModel_torch(dataset['X_train_scaled'].shape[1]).to(device)\n",
    "\n",
    "# Set up the optimizer (Adam) with the model parameters and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Convert training and testing data to PyTorch tensors and move to the specified device\n",
    "x_train = torch.tensor(dataset['X_train_scaled'].values, dtype=torch.float32, device=device)\n",
    "y_train = torch.tensor(dataset['Y_train_scaled'].values, dtype=torch.float32, device=device)\n",
    "x_test = torch.tensor(dataset['X_test_scaled'].values, dtype=torch.float32, device=device)\n",
    "y_test = torch.tensor(dataset['Y_test_scaled'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "# Initialize a DataFrame to store loss information\n",
    "loss_info = pd.DataFrame(columns=['loss train', 'loss test', 'MSE train', 'MSE test', 'MAE train', 'MAE test'])\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 100000\n",
    "patience = 150\n",
    "last_loss = 0\n",
    "batch_size = 50\n",
    "#best_loss = np.inf\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data\n",
    "    x_shuffle, y_shuffle = shuffle(x_train, y_train)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        # Create mini-batches for training\n",
    "        x_batch = x_shuffle[i:i + batch_size]\n",
    "        y_batch = y_shuffle[i:i + batch_size]\n",
    "        # Extract the last three columns as p_batch, T_batch, and M_batch\n",
    "        p_batch = x_batch[:, -1]\n",
    "        T_batch = x_batch[:, -2]\n",
    "        M_batch = x_batch[:, -3]\n",
    "        \n",
    "        # Perform a single optimization step\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "    # Calculate and store metrics for the current epoch\n",
    "    loss_info.loc[epoch, :] = get_metrics(model, x_train, x_test, y_train, y_test, loss_fn)\n",
    "    # Print progress for the current epoch\n",
    "    print_progress(epoch, loss_info, best_loss)\n",
    "    \n",
    "    # Save the model if the current test loss is the best seen so far\n",
    "    if loss_info.loc[epoch, 'loss test'] < best_loss:\n",
    "        torch.save(model.state_dict(), './Models/model_PINN.pt')\n",
    "        best_loss = loss_info.loc[epoch, 'loss test']\n",
    "        last_loss = epoch\n",
    "        \n",
    "    # Early stopping if no improvement in test loss for 'patience' epochs\n",
    "    if epoch - last_loss >= patience:\n",
    "        print('Terminated')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67db932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T09:03:24.737462Z",
     "start_time": "2024-06-12T09:03:24.735628Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2aa52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
