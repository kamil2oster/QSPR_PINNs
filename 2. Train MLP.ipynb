{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f91c15",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will train a simple multilayer perceptron (MLP) model on the data prepared in Notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03604f5",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26394a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:13:15.061716Z",
     "start_time": "2024-06-11T14:13:13.540952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709314c",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Firstly, the data prepared in Notebook 1 are loaded (data combined.csv), including dropping missing values (just in case any creeped in!).\n",
    "\n",
    "Next, we specify the inputs and outputs. Note that to_drop contains any columns that are not to be used in inputs. Inputs will only contain molecular descriptors, M, T and p.\n",
    "Outputs will be density values.\n",
    "\n",
    "Output is divided by 1000 to convert the unit from kg/m3 to g/cm3 (this will be important in PINN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c1ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:13:15.426278Z",
     "start_time": "2024-06-11T14:13:15.062958Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/data combined.csv', index_col = 0).dropna(axis = 0)\n",
    "print('Loaded dataset:')\n",
    "display(df)\n",
    "\n",
    "extra_tags = ['M g/mol', 'T / K', 'p / MPa']\n",
    "to_drop = ['Dataset ID', 'IL ID', 'Cation', 'Anion', 'Cationic family', 'Anionic family',\n",
    "           'Excluded IL', 'Accepted dataset', 'T / K', 'p / MPa', 'ρ / kg/m3', 'SWMLR (v0) + FFANN (f)',\n",
    "           'SWMLR (v0) + FFANN (f).1', 'FFANN (v0) + FFANN (f)', 'FFANN (v0) + FFANN (f).1',\n",
    "           'LSSVM (v0) + FFANN (f)', 'LSSVM (v0) + FFANN (f).1', 'M g/mol']\n",
    "molecular_descriptors = df.drop(to_drop, axis = 1).columns\n",
    "inputs = np.hstack((molecular_descriptors, \n",
    "                    extra_tags))\n",
    "\n",
    "outputs = ['ρ / kg/m3']\n",
    "\n",
    "print(f'\\n\\nInputs {len(inputs)}:\\n{inputs}')\n",
    "print(f'\\n\\nOutputs {len(outputs)}:\\n{outputs}')\n",
    "\n",
    "X = df[inputs]\n",
    "y = df[outputs]/1000\n",
    "\n",
    "print('\\n\\nInput data:')\n",
    "display(X)\n",
    "\n",
    "print('\\n\\nOutput data:')\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8eea6",
   "metadata": {},
   "source": [
    "# Scale and Split Data\n",
    "\n",
    "The code defines two functions, scale and descale, for normalising and denormalising data. The scale function adjusts the input data by subtracting a shift value and dividing by a factor for each column, effectively normalising the data. The descale function reverses this process, restoring the original data values by multiplying by the factor and adding the shift.\n",
    "\n",
    "In the preparation section, a dictionary named scaler is created to store the shift and factor values for both input features (X) and output values (y). These values are used to scale the inputs and outputs into a range defined by scaler_range.\n",
    "\n",
    "The scaled data is then stored in the dataset dictionary, and any columns with all NaN values are dropped. The dataset is split into training and testing sets using a 15% test size, with shapes of the resulting sets printed to confirm the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ddf86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:13:15.524479Z",
     "start_time": "2024-06-11T14:13:15.428102Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(data_in, shift, factor):\n",
    "    \"\"\"\n",
    "    Scale the input data by shifting and dividing by a factor.\n",
    "    \n",
    "    Parameters:\n",
    "    data_in (array-like or DataFrame): The input data to be scaled.\n",
    "    shift (Series or DataFrame): The shift values for each column in the data.\n",
    "    factor (Series or DataFrame): The factor values for each column in the data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The scaled data.\n",
    "    \"\"\"\n",
    "    # Convert the input data to a DataFrame\n",
    "    data_in = pd.DataFrame(data_in)\n",
    "    \n",
    "    # Scale the data by subtracting the shift and dividing by the factor for each column\n",
    "    return (data_in - shift[data_in.columns]) / factor[data_in.columns]\n",
    "\n",
    "def descale(data_in, shift, factor):\n",
    "    \"\"\"\n",
    "    Descale the input data by multiplying by a factor and adding a shift.\n",
    "    \n",
    "    Parameters:\n",
    "    data_in (array-like or DataFrame): The input data to be descaled.\n",
    "    shift (Series or DataFrame): The shift values for each column in the data.\n",
    "    factor (Series or DataFrame): The factor values for each column in the data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The descaled data.\n",
    "    \"\"\"\n",
    "    # Convert the input data to a DataFrame\n",
    "    data_in = pd.DataFrame(data_in)\n",
    "    \n",
    "    # Descale the data by multiplying by the factor and adding the shift for each column\n",
    "    return data_in * factor[data_in.columns] + shift[data_in.columns]\n",
    "\n",
    "# Prepare scaler\n",
    "scaler = {}\n",
    "scaler_range = (0, 1)\n",
    "scaler['X_shift'] = X.min(axis=0)\n",
    "scaler['X_factor'] = X.max(axis=0) - X.min(axis=0)\n",
    "scaler['y_shift'] = y.min(axis=0)\n",
    "scaler['y_factor'] = y.max(axis=0) - y.min(axis=0)\n",
    "\n",
    "# Scale inputs X and output y\n",
    "dataset = {}\n",
    "dataset['X_scaled'] = scale(X, scaler['X_shift'], scaler['X_factor']).dropna(axis = 1, how = 'all')\n",
    "dataset['Y_scaled'] = scale(y, scaler['y_shift'], scaler['y_factor'])\n",
    "print('Scaled input data size:', dataset['X_scaled'].shape)\n",
    "print('Scaled outputs data size:', dataset['Y_scaled'].shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "dataset['X_train_scaled'], dataset['X_test_scaled'], dataset['Y_train_scaled'], dataset['Y_test_scaled'] = train_test_split(dataset['X_scaled'], dataset['Y_scaled'], test_size=0.15, random_state = 23)\n",
    "print('Training size:', dataset['X_train_scaled'].shape, dataset['Y_train_scaled'].shape)\n",
    "print('Testing size:', dataset['X_test_scaled'].shape, dataset['Y_test_scaled'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928dd391",
   "metadata": {},
   "source": [
    "# Define Model and Training Functions\n",
    "\n",
    "The code defines a simple multi-layer perceptron (MLP) model using PyTorch, along with functions for training and evaluating the model.\n",
    "\n",
    "MLPModel_torch Class:\n",
    "\n",
    "* Defines a basic MLP with two hidden layers, each containing 100 neurons, and an output layer with a single neuron for regression tasks.\n",
    "* Uses ReLU activation functions in the hidden layers.\n",
    "* Includes methods for initialisation and forward pass through the network.\n",
    "\n",
    "closure Function:\n",
    "\n",
    "* Performs a single optimisation step, including zeroing the gradients, making predictions, calculating loss, and performing backpropagation.\n",
    "\n",
    "get_metrics Function:\n",
    "\n",
    "* Computes various performance metrics for the model, including training and testing loss, Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n",
    "* Returns the computed metrics as a list.\n",
    "\n",
    "print_progress Function:\n",
    "\n",
    "* Prints the training progress, showing the current epoch's training and testing loss, MSE, and MAE.\n",
    "* Highlights epochs with the best test loss achieved so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f13df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:13:16.718581Z",
     "start_time": "2024-06-11T14:13:16.712269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MLPModel_torch(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple multi-layer perceptron (MLP) model using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dim (int): The number of input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPModel_torch, self).__init__()\n",
    "        # Define the first fully connected layer with 100 neurons\n",
    "        self.dense1 = nn.Linear(input_dim, 100)\n",
    "        # Define the second fully connected layer with 100 neurons\n",
    "        self.dense2 = nn.Linear(100, 100)\n",
    "        # Define the output layer with 1 neuron for regression output\n",
    "        self.dense_density = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "        Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Apply ReLU activation function to the output of the first layer\n",
    "        x = torch.relu(self.dense1(x))\n",
    "        # Apply ReLU activation function to the output of the second layer\n",
    "        x = torch.relu(self.dense2(x))\n",
    "        # Generate the final output\n",
    "        y = self.dense_density(x)\n",
    "        return y\n",
    "\n",
    "def closure():\n",
    "    \"\"\"\n",
    "    Perform a single optimization step for the model.\n",
    "    \n",
    "    Returns:\n",
    "    Tensor: The loss value.\n",
    "    \"\"\"\n",
    "    # Zero the gradients of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x_batch to the model\n",
    "    y_pred = model(x_batch)\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(y_pred, y_batch).mean()\n",
    "    # Backward pass: compute gradient of the loss with respect to parameters\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def get_metrics(model, X_train, X_test, y_train, y_test, loss_fn):\n",
    "    \"\"\"\n",
    "    Calculate and return various performance metrics for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    model (nn.Module): The trained model.\n",
    "    X_train (Tensor): Training input data.\n",
    "    X_test (Tensor): Testing input data.\n",
    "    y_train (Tensor): Training target data.\n",
    "    y_test (Tensor): Testing target data.\n",
    "    loss_fn (function): Loss function.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of metrics [train loss, test loss, train MSE, test MSE, train MAE, test MAE].\n",
    "    \"\"\"\n",
    "    # Predict on the training data\n",
    "    y_pred_train = model(X_train)\n",
    "    # Compute the training loss\n",
    "    loss_train = loss_fn(y_pred_train, y_train)\n",
    "    \n",
    "    # Predict on the testing data\n",
    "    y_pred_test = model(X_test)\n",
    "    # Compute the testing loss\n",
    "    loss_test = loss_fn(y_pred_test, y_test)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) for training data\n",
    "    MSE = torch.mean((y_pred_train - y_train) ** 2)\n",
    "    # Calculate Mean Absolute Error (MAE) for training data\n",
    "    MAE = torch.mean(torch.abs(y_pred_train - y_train))\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) for testing data\n",
    "    MSE_test = torch.mean((y_pred_test - y_test) ** 2)\n",
    "    # Calculate Mean Absolute Error (MAE) for testing data\n",
    "    MAE_test = torch.mean(torch.abs(y_pred_test - y_test))\n",
    "    \n",
    "    # Return the computed metrics as a list\n",
    "    return [loss_train.mean().item(), \n",
    "            loss_test.mean().item(), \n",
    "            MSE.item(), \n",
    "            MSE_test.item(),\n",
    "            MAE.item(),\n",
    "            MAE_test.item()]\n",
    "\n",
    "def print_progress(epoch, loss_info, best_loss):\n",
    "    \"\"\"\n",
    "    Print the progress of training, including various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    epoch (int): The current epoch number.\n",
    "    loss_info (DataFrame): DataFrame containing the loss information.\n",
    "    best_loss (float): The best loss achieved so far.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the current epoch's test loss is better than the best loss\n",
    "    if loss_info.loc[epoch, 'loss test'] < best_loss:\n",
    "        # Print progress with an asterisk indicating a new best test loss\n",
    "        print('{}   {:.3e}/{:.3e}   {:.3e}/{:.3e}   {:.3e}/{:.3e} *'.format(epoch,\n",
    "            loss_info.loc[epoch, 'loss train'],\n",
    "            loss_info.loc[epoch, 'loss test'],\n",
    "            loss_info.loc[epoch, 'MSE train'], \n",
    "            loss_info.loc[epoch, 'MSE test'],\n",
    "            loss_info.loc[epoch, 'MAE train'], \n",
    "            loss_info.loc[epoch, 'MAE test']))\n",
    "    else:\n",
    "        # Print progress without an asterisk\n",
    "        print('{}   {:.3e}/{:.3e}   {:.3e}/{:.3e}   {:.3e}/{:.3e}    {}'.format(epoch,\n",
    "            loss_info.loc[epoch, 'loss train'],\n",
    "            loss_info.loc[epoch, 'loss test'],\n",
    "            loss_info.loc[epoch, 'MSE train'], \n",
    "            loss_info.loc[epoch, 'MSE test'],\n",
    "            loss_info.loc[epoch, 'MAE train'], \n",
    "            loss_info.loc[epoch, 'MAE test'],\n",
    "            epoch - last_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee701ef",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "This code provides a detailed workflow for training and evaluating a multi-layer perceptron (MLP) model using PyTorch, focusing on optimizing model performance and implementing early stopping.\n",
    "\n",
    "Firstly, the device is set to use GPU ('cuda') if available, allowing for faster computation. An instance of the MLPModel_torch class is then created and moved to the specified device. The Adam optimizer is set up with the model parameters and a learning rate of 0.0001, and the loss function is defined as Mean Squared Error (MSE) without reduction.\n",
    "\n",
    "The training and testing data are converted to PyTorch tensors and moved to the specified device. A DataFrame is initialized to store loss information, including training and testing loss, Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n",
    "\n",
    "Several training parameters are defined: the number of epochs is set to 100,000, with early stopping patience set to 150 epochs. Initial values for the last improved epoch and best loss are set, and the batch size is defined as 50.\n",
    "\n",
    "The training loop begins with shuffling the training data for each epoch. Mini-batch training is performed by dividing the data into batches and performing optimization steps on each batch. After each epoch, performance metrics are calculated and stored. The training progress is printed, highlighting if the current epoch achieved the best test loss so far. The model is saved if the current test loss is the best seen so far. Early stopping is implemented to terminate training if no improvement in test loss is observed for a specified number of epochs (patience).\n",
    "\n",
    "This workflow ensures efficient training with proper evaluation, progress tracking, and early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98084033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T14:14:29.713490Z",
     "start_time": "2024-06-11T14:13:23.838400Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = 'cuda'\n",
    "\n",
    "# Initialize the model and move it to the specified device\n",
    "model = MLPModel_torch(dataset['X_train_scaled'].shape[1]).to(device)\n",
    "\n",
    "# Set up the optimizer (Adam) with the model parameters and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the loss function (Mean Squared Error) without reduction\n",
    "loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "# Convert training and testing data to PyTorch tensors and move to the specified device\n",
    "x_train = torch.tensor(dataset['X_train_scaled'].values, dtype=torch.float32, device=device)\n",
    "y_train = torch.tensor(dataset['Y_train_scaled'].values, dtype=torch.float32, device=device)\n",
    "x_test = torch.tensor(dataset['X_test_scaled'].values, dtype=torch.float32, device=device)\n",
    "y_test = torch.tensor(dataset['Y_test_scaled'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "# Initialize a DataFrame to store loss information\n",
    "loss_info = pd.DataFrame(columns=['loss train', 'loss test', 'MSE train', 'MSE test', 'MAE train', 'MAE test'])\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 100000\n",
    "patience = 150\n",
    "last_loss = 0\n",
    "batch_size = 50\n",
    "best_loss = np.inf\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data\n",
    "    x_shuffle, y_shuffle = shuffle(x_train, y_train)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        x_batch = x_shuffle[i:i + batch_size]\n",
    "        y_batch = y_shuffle[i:i + batch_size]\n",
    "        # Perform a single optimization step\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    # Calculate and store metrics for the current epoch\n",
    "    loss_info.loc[epoch, :] = get_metrics(model, x_train, x_test, y_train, y_test, loss_fn)\n",
    "    # Print progress for the current epoch\n",
    "    print_progress(epoch, loss_info, best_loss)\n",
    "    \n",
    "    # Save the model if the current test loss is the best seen so far\n",
    "    if loss_info.loc[epoch, 'loss test'] < best_loss:\n",
    "        torch.save(model.state_dict(), './Models/model_MLP.pt')\n",
    "        best_loss = loss_info.loc[epoch, 'loss test']\n",
    "        last_loss = epoch\n",
    "        \n",
    "    # Early stopping if no improvement in test loss for 'patience' epochs\n",
    "    if epoch - last_loss >= patience:\n",
    "        print('Terminated')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
